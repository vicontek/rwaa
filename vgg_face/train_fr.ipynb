{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.resnet as ResNet\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "import json\n",
    "import numpy as np\n",
    "import imdirect\n",
    "import numpy as n\n",
    "import torch\n",
    "import gc\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "%matplotlib inline\n",
    "\n",
    "# !!!!  code from https://github.com/cydonia999/VGGFace2-pytorch/  !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOADED_WEIGHTS_PATH = '/home/tva/edu/sk/rwaa/resnet50_ft_weight.pkl'\n",
    "\n",
    "# To train from vggface2 pretrained model you need to download weights from the github repository\n",
    "net = ResNet.resnet50(num_classes=8631, include_top=True)\n",
    "utils.load_state_dict(net, DOWNLOADED_WEIGHTS_PATH) # download weights from github above\n",
    "net.fc = nn.Linear(512 * 4, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to train only last layer\n",
    "def get_parameters(model, bias=False):\n",
    "    for k, m in model._modules.items():\n",
    "        if k == \"fc\" and isinstance(m, nn.Linear):\n",
    "            if bias:\n",
    "                yield m.bias\n",
    "            else:\n",
    "                yield m.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, root_path, train_test_path, mode='train', transform=None):\n",
    "        self.markup = []\n",
    "        self.root_path = root_path\n",
    "        self.transform = transform\n",
    "        self.mean_bgr = np.array([91.4953, 103.8827, 131.0912])  # values taken from source repo. Net was pretrained with them\n",
    "        \n",
    "        with open(train_test_path, 'r') as f:\n",
    "            self.train_test_split = json.load(f)\n",
    "        \n",
    "        self.ordered_classes = sorted(list(self.train_test_split.keys()))\n",
    "        self.class_marks = {name: i for i, name in enumerate(self.ordered_classes)}\n",
    "        \n",
    "        for i, person in enumerate(self.ordered_classes):\n",
    "            self.markup.extend([\n",
    "                (str((Path(self.root_path) / person / img_name).absolute()), i)\n",
    "                for img_name in self.train_test_split[person][mode]\n",
    "            ])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path, label = self.markup[index]\n",
    "        img = Image.open(img_path)\n",
    "        img = torchvision.transforms.Resize((224, 224))(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        img = np.array(img)\n",
    "        img = img[:, :, ::-1]  # RGB -> BGR\n",
    "        img = img.astype(np.float32)\n",
    "        img -= self.mean_bgr\n",
    "        img = img.transpose(2, 0, 1)  # C x H x W        \n",
    "        img = torch.from_numpy(img).float()\n",
    "\n",
    "        \n",
    "        return img, label\n",
    "    \n",
    "    \n",
    "    def __len__(self,):\n",
    "        return len(self.markup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some training time augmentations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation((-5, 5)),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIGNED_DATASET_PATH = '/home/tva/edu/sk/aligned_dev20/'\n",
    "TRAIN_TEST_SPLIT_PATH = '/home/tva/edu/sk/train_test_markup.json'\n",
    "\n",
    "train_dataset = FaceDataset(root_path=ALIGNED_DATASET_PATH,\n",
    "                            train_test_path=TRAIN_TEST_SPLIT_PATH,\n",
    "                            mode='train',\n",
    "                            transform=train_transform)\n",
    "\n",
    "test_dataset = FaceDataset(root_path=ALIGNED_DATASET_PATH,\n",
    "                            train_test_path=TRAIN_TEST_SPLIT_PATH,\n",
    "                            mode='test',\n",
    "                            transform=None)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=40, shuffle=True, num_workers=3)\n",
    "val_loader = DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to specify num_classes=8631 to load pretrained weights, because the model was trained on that amount of classes\n",
    "# Just replace the last FullyConnected layer with a new one\n",
    "net = ResNet.resnet50(num_classes=8631, include_top=True)\n",
    "utils.load_state_dict(net, '/home/tva/edu/sk/rwaa/resnet50_ft_weight.pkl') # download weights from github above\n",
    "net.fc = nn.Linear(512 * 4, 20)\n",
    "\n",
    "\n",
    "## Or you can just load our own pretrained weights\n",
    "net = ResNet.resnet50(num_classes=20, include_top=True)\n",
    "net.load_state_dict(torch.load('finetuned.pth'))\n",
    "\n",
    "\n",
    "# device can be either 'cuda' or 'cpu'. Define it once and use .to(device) everywhere \n",
    "device = torch.device('cuda')\n",
    "net.to(device)\n",
    "\n",
    "lf = nn.CrossEntropyLoss()\n",
    "lf = lf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "\n",
    "args = types.SimpleNamespace()\n",
    "args.print_freq = 100\n",
    "args.lr = 0.1\n",
    "args.lr_update_freq = 1000  # update lr every ... iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(\n",
    "            [\n",
    "                {'params': get_parameters(net, bias=False)},\n",
    "                {'params': get_parameters(net, bias=True), 'lr': args.lr * 2, 'weight_decay': 0},\n",
    "            ],\n",
    "            lr=args.lr,)\n",
    "\n",
    "# optim = torch.optim.Adam(net.parameters(),\n",
    "#             lr=args.lr,)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optim, args.lr_update_freq, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# A function to evaluate performance on the validation set \n",
    "def validate(training=True):\n",
    "    best_top1 = 0.0\n",
    "    best_top5 = 0.0\n",
    "    batch_time = utils.AverageMeter()\n",
    "    losses = utils.AverageMeter()\n",
    "    top1 = utils.AverageMeter()\n",
    "    top5 = utils.AverageMeter()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    iteration = 0\n",
    "    for batch_idx, (imgs, target) in tqdm.tqdm(\n",
    "            enumerate(val_loader), total=len(val_loader),\n",
    "            desc='Valid iteration={}'.format(iteration), ncols=80, leave=False):\n",
    "\n",
    "        gc.collect()\n",
    "        iteration += 1\n",
    "        imgs, target = imgs.to(device), target.to(device)\n",
    "            \n",
    "        output = net(imgs)\n",
    "        loss = lf(output, target)\n",
    "\n",
    "        if np.isnan(float(loss.item())):\n",
    "            raise ValueError('loss is nan while validating')\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = utils.accuracy(output.data, target.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), imgs.size(0))\n",
    "        top1.update(prec1[0], imgs.size(0))\n",
    "        top5.update(prec5[0], imgs.size(0))\n",
    "\n",
    "    if training == True:\n",
    "        is_best = top1.avg > best_top1\n",
    "        best_top1 = max(top1.avg, best_top1)\n",
    "        best_top5 = max(top5.avg, best_top5)\n",
    "\n",
    "        log_str = 'Test_summary: [{0}/{1}/{top1.count:}] iter: {iteration:}\\t' \\\n",
    "              'BestPrec@1: {best_top1:.3f}\\tBestPrec@5: {best_top5:.3f}\\t' \\\n",
    "              'Loss: {loss.avg:.4f}\\t' \\\n",
    "              'Prec@1: {top1.avg:.3f}\\tPrec@5: {top5.avg:.3f}\\t'.format(\n",
    "            batch_idx, len(val_loader), iteration=iteration,\n",
    "            best_top1=best_top1, best_top5=best_top5,\n",
    "           loss=losses, top1=top1, top5=top5)\n",
    "        print(log_str)\n",
    "\n",
    "        if training:\n",
    "            net.train()\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch=0, iter=0:  10%|██                   | 2/20 [00:00<00:08,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0/20/40]\tepoch: 0\titer: 0\tLoss: 0.0000 (0.0000)\tPrec@1: 100.000 (100.000)\tPrec@5: 100.000 (100.000)\tlr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_summary: [19/20/781]\tepoch: 0\titer: 19\tTime: 0.000\tLoss: 0.3056\tPrec@1: 99.872\tPrec@5: 100.000\tlr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9c6dfd8ee570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss is nan while training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 247, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 247, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 247, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "batch_time = utils.AverageMeter()\n",
    "losses = utils.AverageMeter()\n",
    "top1 = utils.AverageMeter()  # top1 precision\n",
    "top5 = utils.AverageMeter()  # top5 precision\n",
    "\n",
    "\n",
    "# train loop        \n",
    "for epoch in range(1000000):\n",
    "    iteration = 0\n",
    "    for batch_idx, (imgs, target) in tqdm.tqdm(\n",
    "                    enumerate(train_loader), total=len(train_loader),\n",
    "                    desc='Train epoch={}, iter={}'.format(epoch, iteration), ncols=80, leave=False):\n",
    "        iteration = batch_idx + epoch * len(train_loader)\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = net(imgs)\n",
    "        loss = lf(output, target)\n",
    "\n",
    "        if np.isnan(float(loss.item())):\n",
    "            raise ValueError('loss is nan while training')\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = utils.accuracy(output.data, target.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), imgs.size(0))\n",
    "        top1.update(prec1[0], imgs.size(0))\n",
    "        top5.update(prec5[0], imgs.size(0))\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if iteration % args.print_freq == 0:\n",
    "            log_str = 'Train: [{0}/{1}/{top1.count:}]\\tepoch: {epoch:}\\titer: {iteration:}\\t' \\\n",
    "                  'Loss: {loss.val:.4f} ({loss.avg:.4f})\\t' \\\n",
    "                  'Prec@1: {top1.val:.3f} ({top1.avg:.3f})\\t' \\\n",
    "                  'Prec@5: {top5.val:.3f} ({top5.avg:.3f})\\tlr {lr:.6f}'.format(\n",
    "                batch_idx, len(train_loader), epoch=epoch, iteration=iteration,\n",
    "                lr=optim.param_groups[0]['lr'],\n",
    "                batch_time=batch_time, loss=losses, top1=top1, top5=top5)\n",
    "            print(log_str)\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()  # update lr\n",
    "\n",
    "\n",
    "    log_str = 'Train_summary: [{0}/{1}/{top1.count:}]\\tepoch: {epoch:}\\titer: {iteration:}\\t' \\\n",
    "                  'Time: {batch_time.avg:.3f}\\t' \\\n",
    "                  'Loss: {loss.avg:.4f}\\tPrec@1: {top1.avg:.3f}\\tPrec@5: {top5.avg:.3f}\\tlr {lr:.6f}'.format(\n",
    "                batch_idx, len(train_loader), epoch=epoch, iteration=iteration,\n",
    "                lr=optim.param_groups[0]['lr'],\n",
    "                batch_time=batch_time, loss=losses, top1=top1, top5=top5)\n",
    "    print(log_str)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'finetuned2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_summary: [17/18/175] iter: 18\tBestPrec@1: 100.000\tBestPrec@5: 100.000\tLoss: 0.0000\tPrec@1: 100.000\tPrec@5: 100.000\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "validate(training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tva/.local/lib/python3.5/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        1.7001e-33, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2612e-44,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3177e-33, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.functional.F.softmax(net(test_dataset[1][0].unsqueeze(0).to(device)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'vgg_face'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-313790257600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mvgg_face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'vgg_face'"
     ]
    }
   ],
   "source": [
    "import vgg_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
